#-*-coding:utf-8-*-
'''
Created on Oct 24,2018

@author: pengzhiliang
'''
import os
from os.path import join as pjoin
import collections
import json
import torch
import numpy as np
import scipy.misc as m
import scipy.io as io
import matplotlib.pyplot as plt
import glob

from PIL import Image
# Tqdm æ˜¯ä¸€ä¸ªå¿«é€Ÿï¼Œå¯æ‰©å±•çš„Pythonè¿›åº¦æ¡ï¼Œå¯ä»¥åœ?Python é•¿å¾ªç¯ä¸­æ·»åŠ ä¸€ä¸ªè¿›åº¦æç¤ºä¿¡æ¯ï¼Œç”¨æˆ·åªéœ€è¦å°è£…ä»»æ„çš„è¿­ä»£å™?tqdm(iterator)
from tqdm import tqdm

from torch.utils import data
from torchvision import transforms

"""
Pascal Voc 2012 & benchmark_release æ•°æ®é›†ä»‹ç»ï¼š
è¯¦æƒ…è¯·è§ï¼šhttps://blog.csdn.net/iamoldpan/article/details/79196413

VOC2012æ•°æ®é›†åˆ†ä¸?0ç±»ï¼ŒåŒ…æ‹¬èƒŒæ™¯ä¸?1ç±»ï¼Œåˆ†åˆ«å¦‚ä¸‹ï¼?
- Person: person 
- Animal: bird, cat, cow, dog, horse, sheep 
- Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train 
- Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor

VOC2012ä¸­çš„å›¾ç‰‡å¹¶ä¸æ˜¯éƒ½ç”¨äºåˆ†å‰²ï¼Œç”¨äºåˆ†å‰²æ¯”èµ›çš„å›¾ç‰‡åŒ…å«åŸå›¾ã€å›¾åƒåˆ†ç±»åˆ†å‰²å’Œå›¾åƒç‰©ä½“åˆ†å‰²ä¸¤ç§pngå›¾ã€?å›¾åƒåˆ†ç±»åˆ†å‰²ï¼šåœ¨20ç§ç‰©ä½“ä¸­ï¼Œground-turthå›¾ç‰‡ä¸Šæ¯ä¸ªç‰©ä½“çš„è½®å»“å¡«å……éƒ½æœ‰ä¸€ä¸ªç‰¹å®šçš„é¢œè‰²ï¼Œä¸€å…?0ç§é¢œè‰²ï¼Œæ¯”å¦‚æ‘©æ‰˜è½¦ç”¨çº¢è‰²è¡¨ç¤ºï¼Œäººç”¨ç»¿è‰²è¡¨ç¤ºã€?å›¾åƒç‰©ä½“åˆ†å‰²ï¼šåˆ™ä»…ä»…åœ¨ä¸€å‰¯å›¾ä¸­ç”Ÿæˆä¸åŒç‰©ä½“çš„è½®å»“é¢œè‰²å³å¯ï¼Œé¢œè‰²è‡ªå·±éšä¾¿å¡«å……ã€?
åœ¨FCNè¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç”¨åˆ°çš„æ•°æ®é›†å³æ˜¯åŸºæœ¬çš„åˆ†å‰²æ•°æ®é›†ï¼Œä¸€å…±æœ‰ä¸¤å¥—åˆ†åˆ«æ˜¯benchmark_RELEASEå’ŒVOC2012

å›¾åƒåˆ†å‰²çš„æ•°æ®é›†ä¸€èˆ¬éƒ½æ˜¯é‡‡ç”¨ä¸Šé¢è¯´æ˜çš„VOC2012æŒ‘æˆ˜æ•°æ®é›†ï¼Œæœ‰äººè¯´benchmark_LELEASEä¸ºå¢å¼ºæ•°æ®é›†ï¼Œå…·ä½“åŸå› æˆ‘ä¸æ¸…æ¥šï¼Œ
å¯èƒ½æ˜¯å› ä¸ºbenchmark_LELEASEçš„å›¾ç‰‡éƒ½æ˜¯ç”¨äºåˆ†å‰²ï¼ˆä¸€å…?1355å¼ ï¼‰ï¼Œè€ŒVOC2012ä»…ä»…éƒ¨åˆ†å›¾ç‰‡é€‚ç”¨äºåˆ†å‰²ï¼ˆ2913å¼ ï¼‰å?"""
def get_data_path(name):
    """Extract path to data from config file.

    Args:
        name (str): The name of the dataset.

    Returns:
        (str): The path to the root directory containing the dataset.
    """
    js = open("datapath.json").read()
    data = json.loads(js)
    return os.path.expanduser(data[name]["data_path"])


class pascalVOCLoader(data.Dataset):
    """Data loader for the Pascal VOC semantic segmentation dataset.
    
    Download:http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
    Download: http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/semantic_contours/benchmark.tgz

    æ¥è‡ªVOCå’ŒSBDä¸¤ä¸ªæ•°æ®é›†çš„Annotationsï¼ˆåœ¨VOCä¸­æ˜¯RGBå›¾åƒä¸”é¢œè‰²ä»£è¡¨ç‰¹å®šçš„ç±»ï¼Œåœ¨SBDä¸­æ˜¯.matçš„æ ¼å¼ï¼‰
    è½¬æ¢ä¸ºé€šç”¨çš„â€œlabel_maskâ€æ ¼å¼ã€?åœ¨æ­¤æ ¼å¼ä¸‹ï¼Œæ¯ä¸ªæ©ç æ˜?åˆ?1ä¹‹é—´çš„æ•´æ•°å€¼çš„ï¼ˆMï¼ŒNï¼‰æ•°ç»„ï¼Œå…¶ä¸­0è¡¨ç¤ºèƒŒæ™¯ç±»ã€?   
    label maskså­˜å‚¨åœ¨ä¸€ä¸ªåä¸º`pre_encoded`çš„æ–°æ–‡ä»¶å¤¹ä¸­ï¼?    è¯¥æ–‡ä»¶å¤¹ä½œä¸ºåŸå§‹Pascal VOCæ•°æ®ç›®å½•ä¸­`SegmentationClass`æ–‡ä»¶å¤¹çš„å­ç›®å½•æ·»åŠ ã€?
    A total of five data splits are provided for working with the VOC data:
        train: The original VOC 2012 training data - 1464 images
        val: The original VOC 2012 validation data - 1449 images
        trainval: The combination of `train` and `val` - 2913 images
        train_aug: The unique images present in both the train split and
                   training images from SBD: - 8829 images (the unique members
                   of the result of combining lists of length 1464 and 8498)
        train_aug_val: The original VOC 2012 validation data minus the images
                   present in `train_aug` (This is done with the same logic as
                   the validation set used in FCN PAMI paper, but with VOC 2012
                   rather than VOC 2011) - 904 images
    """

    def __init__(self,root,split="train_aug",is_transform=False,img_size=512,augmentations=None,img_norm=True):
        self.root = os.path.expanduser(root)
        self.split = split
        self.is_transform = is_transform
        self.augmentations = augmentations
        self.img_norm = img_norm
        self.n_classes = 21
        self.mean = np.array([104.00699, 116.66877, 122.67892])
        self.files = collections.defaultdict(list)
        self.img_size = (
            img_size if isinstance(img_size, tuple) else (img_size, img_size)
        )
        for split in ["train", "val", "trainval"]:
            # åˆ†åˆ«è¯»å–VOC2012/ImageSets/Segmentationä¸‹çš„ä¸‰ä¸ªæ–‡ä»¶åˆ—è¡¨ï¼ŒåŒ…å«è®­ç»ƒä¿¡æ¯ï¼Œå¹¶ç”¨å­—å…¸ä¿å­˜
            path = pjoin(self.root, "ImageSets/Segmentation", split + ".txt")
            file_list = tuple(open(path, "r"))
            file_list = [id_.rstrip() for id_ in file_list]
            self.files[split] = file_list
        self.setup_annotations()
        self.tf = transforms.Compose([transforms.ToTensor(),
                                      transforms.Normalize([0.485, 0.456, 0.406], 
                                                           [0.229, 0.224, 0.225])])

    def __len__(self):
        # è¿”å›æ‰€é€‰data splitsé•¿åº¦ï¼Œå³ä¸åŒè®­ç»ƒé›†è¿”å›ç›¸åº”çš„è®­ç»ƒæ•°æ®ä¸ªæ•°
        return len(self.files[self.split])

    def __getitem__(self, index):
        im_name = self.files[self.split][index]
        im_path = pjoin(self.root, "JPEGImages", im_name + ".jpg")
        lbl_path = pjoin(self.root, "SegmentationClass/pre_encoded", im_name + ".png")
        im = Image.open(im_path)
        lbl = Image.open(lbl_path)
        if self.augmentations is not None:
            im, lbl = self.augmentations(im, lbl)
        if self.is_transform:
            im, lbl = self.transform(im, lbl)
        return im, lbl

    def transform(self, img, lbl):
        if self.img_size == ('same', 'same'):
            pass
        else:
            img = img.resize((self.img_size[0], self.img_size[1]))  # uint8 with RGB mode
            lbl = lbl.resize((self.img_size[0], self.img_size[1]))
        img = self.tf(img)
        lbl = torch.from_numpy(np.array(lbl)).long()
        lbl[lbl == 255] = 0
        return img, lbl

    def get_pascal_labels(self):
        """Load the mapping that associates pascal classes with label colors

        Returns:
            np.ndarray with dimensions (21, 3)
        """
        return np.asarray(
            [
                [0, 0, 0],
                [128, 0, 0],
                [0, 128, 0],
                [128, 128, 0],
                [0, 0, 128],
                [128, 0, 128],
                [0, 128, 128],
                [128, 128, 128],
                [64, 0, 0],
                [192, 0, 0],
                [64, 128, 0],
                [192, 128, 0],
                [64, 0, 128],
                [192, 0, 128],
                [64, 128, 128],
                [192, 128, 128],
                [0, 64, 0],
                [128, 64, 0],
                [0, 192, 0],
                [128, 192, 0],
                [0, 64, 128],
            ]
        )

    def encode_segmap(self, mask):
        """Encode segmentation label images as pascal classes
        ç¼–ç maskï¼Œå°†é¢œè‰²è½¬æ¢ä¸ºç±»åˆ«æ ‡è®?        Args:
            mask (np.ndarray): raw segmentation label image of dimension
              (M, N, 3), in which the Pascal classes are encoded as colours.

        Returns:
            (np.ndarray): class map with dimensions (M,N), where the value at
            a given location is the integer denoting the class index.
        """
        mask = mask.astype(int)
        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)
        for ii, label in enumerate(self.get_pascal_labels()):
            label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii
        label_mask = label_mask.astype(int)
        return label_mask

    def decode_segmap(self, label_mask, plot=False):
        """Decode segmentation class labels into a color image
        è§£ç mask,ç±»åˆ«è½¬æ¢ä¸ºé¢œè‰²ï¼Œæ–¹ä¾¿æ˜¾ç¤º
        Args:
            label_mask (np.ndarray): an (M,N) array of integer values denoting
              the class label at each spatial location.
            plot (bool, optional): whether to show the resulting color image
              in a figure.

        Returns:
            (np.ndarray, optional): the resulting decoded color image.
        """
        label_colours = self.get_pascal_labels()
        r = label_mask.copy()
        g = label_mask.copy()
        b = label_mask.copy()
        for ll in range(0, self.n_classes):
            r[label_mask == ll] = label_colours[ll, 0]
            g[label_mask == ll] = label_colours[ll, 1]
            b[label_mask == ll] = label_colours[ll, 2]
        rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))
        rgb[:, :, 0] = r / 255.0
        rgb[:, :, 1] = g / 255.0
        rgb[:, :, 2] = b / 255.0
        if plot:
            plt.imshow(rgb)
            plt.show()
        else:
            return rgb

    def setup_annotations(self):
        """
        å°†benmarkä¸­çš„è®­ç»ƒæ•°æ®åŠ å…¥
        Sets up Berkley annotations by adding image indices to the
        `train_aug` split and pre-encode all segmentation labels into the
        common label_mask format (if this has not already been done). This
        function also defines the `train_aug` and `train_aug_val` data splits
        according to the description in the class docstring
        """
        sbd_path = get_data_path("sbd")
        target_path = pjoin(self.root, "SegmentationClass/pre_encoded")
        if not os.path.exists(target_path):
            os.makedirs(target_path)
        path = pjoin(sbd_path, "dataset/train.txt")
        sbd_train_list = tuple(open(path, "r"))
        sbd_train_list = [id_.rstrip() for id_ in sbd_train_list]
        train_aug = self.files["train"] + sbd_train_list

        # keep unique elements (stable)
        train_aug = [
            train_aug[i] for i in sorted(np.unique(train_aug, return_index=True)[1])
        ]
        self.files["train_aug"] = train_aug
        set_diff = set(self.files["val"]) - set(train_aug)  # remove overlap
        self.files["train_aug_val"] = list(set_diff)

        pre_encoded = glob.glob(pjoin(target_path, "*.png"))
        expected = np.unique(self.files["train_aug"] + self.files["val"]).size

        if len(pre_encoded) != expected:
            print("Pre-encoding segmentation masks...")
            for ii in tqdm(sbd_train_list):
                lbl_path = pjoin(sbd_path, "dataset/cls", ii + ".mat")
                data = io.loadmat(lbl_path)
                lbl = data["GTcls"][0]["Segmentation"][0].astype(np.int32)
                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())
                m.imsave(pjoin(target_path, ii + ".png"), lbl)

            for ii in tqdm(self.files["trainval"]):
                fname = ii + ".png"
                lbl_path = pjoin(self.root, "SegmentationClass", fname)
                lbl = self.encode_segmap(m.imread(lbl_path))
                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())
                m.imsave(pjoin(target_path, fname), lbl)

        assert expected == 9733, "unexpected dataset sizes"


# Leave code for debugging purposes
# import ptsemseg.augmentations as aug
# if __name__ == '__main__':
# # local_path = '/home/meetshah1995/datasets/VOCdevkit/VOC2012/'
# bs = 4
# augs = aug.Compose([aug.RandomRotate(10), aug.RandomHorizontallyFlip()])
# dst = pascalVOCLoader(root=local_path, is_transform=True, augmentations=augs)
# trainloader = data.DataLoader(dst, batch_size=bs)
# for i, data in enumerate(trainloader):
# imgs, labels = data
# imgs = imgs.numpy()[:, ::-1, :, :]
# imgs = np.transpose(imgs, [0,2,3,1])
# f, axarr = plt.subplots(bs, 2)
# for j in range(bs):
# axarr[j][0].imshow(imgs[j])
# axarr[j][1].imshow(dst.decode_segmap(labels.numpy()[j]))
# plt.show()
# a = raw_input()
# if a == 'ex':
# break
# else:
# plt.close()
